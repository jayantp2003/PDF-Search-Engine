{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jayantparakh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/jayantparakh/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jayantparakh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import nltk\n",
    "import math\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(file_path='Report.pdf',extract_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.lazy_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object PyPDFLoader.lazy_load at 0x130d7adc0>\n"
     ]
    }
   ],
   "source": [
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs1=[]\n",
    "for doc in docs:\n",
    "    docs1.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(content):\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    twords = word_tokenize(text=content.lower())\n",
    "    twords = [lemmatizer.lemmatize(word) for word in twords if word.isalpha() and word not in stopword]\n",
    "    word_freq = Counter(twords)\n",
    "    word_freq_dict = dict(word_freq)\n",
    "    return word_freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wordfreqs={}\n",
    "for i in docs1:\n",
    "    wordfreqs[i.metadata['page']]=preprocess(i.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank(query):\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    twords = word_tokenize(text=query.lower())\n",
    "    twords = [lemmatizer.lemmatize(word) for word in twords if word.isalpha() and word not in stopword]\n",
    "    sum1 = {word:0 for word in twords}\n",
    "    pagefreq = {word:0.0001 for word in twords}\n",
    "    for word in twords:\n",
    "        for i in wordfreqs:\n",
    "            if word in wordfreqs[i].keys():\n",
    "                sum1[word]+=wordfreqs[i][word]\n",
    "                pagefreq[word]+=1\n",
    "    tf = {}\n",
    "    for word in twords:\n",
    "        tf[word] = {}          \n",
    "        for i in wordfreqs:\n",
    "            if word in wordfreqs[i].keys():\n",
    "                tf[word][i]=(wordfreqs[i][word]/sum1[word])\n",
    "            else:\n",
    "                tf[word][i]=0\n",
    "\n",
    "\n",
    "    idf = {}\n",
    "    totalpages = len(wordfreqs.keys())\n",
    "    for word in twords:\n",
    "        idf[word] = math.log(totalpages/pagefreq[word])\n",
    "\n",
    "    tfidfscore = {}\n",
    "    pagerankscore = {i:0 for i in range(totalpages)}\n",
    "    for word in twords:\n",
    "        tfidfscore[word]={}\n",
    "        for i in range(totalpages):\n",
    "             tfidfscore[word][i]=tf[word][i]*idf[word]\n",
    "             pagerankscore[i]+=tfidfscore[word][i]\n",
    "    return pagerankscore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = pagerank(\"oracle internship automation spring boot framework is am are was were has\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.13095888851712675,\n",
       " 1: 0.26902977546087714,\n",
       " 2: 0.9547612630071851,\n",
       " 3: 0.08696362169794256,\n",
       " 4: 0.34141921851163437,\n",
       " 5: 0.2647317092739687,\n",
       " 6: 0.1313654249572978,\n",
       " 7: 0.16977951506925784,\n",
       " 8: 0.2627308499145956,\n",
       " 9: 0.1461846539181808,\n",
       " 10: 0.05614109396167742,\n",
       " 11: 0.008003437437492513,\n",
       " 12: 0.5121057896752825}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: 1, 12: 2, 4: 3, 1: 4, 5: 5, 8: 6, 7: 7, 9: 8, 6: 9, 0: 10, 3: 11, 10: 12, 11: 13}\n"
     ]
    }
   ],
   "source": [
    "scores = ranks\n",
    "sorted_indices = sorted(scores, key=scores.get, reverse=True)\n",
    "rankings = {index: rank + 1 for rank, index in enumerate(sorted_indices)}\n",
    "print(rankings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
